{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1523933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba139ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "    \"\"\"\n",
    "    A linear transformation.\n",
    "\n",
    "        y = xA + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, *, rngs: nnx.Rngs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim : dimension of the input vector\n",
    "            output_dim : dimension of the output vector\n",
    "        \"\"\"\n",
    "        self.A = nnx.Param(rngs.params.uniform((input_dim, output_dim)))\n",
    "        self.b = nnx.Param(jnp.zeros(output_dim))\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        return x @ self.A + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72809ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear(2, 5, rngs=nnx.Rngs(params=0))(x=jnp.ones((2, 4, 3, 2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e14d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nnx.Module):\n",
    "    \"\"\"\n",
    "    Map discrete tokens to an embedding space.\n",
    "\n",
    "        token i --> A[i]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
    "        self.A = nnx.Param(rngs.params.uniform((vocab_size, embed_dim)))\n",
    "\n",
    "    def __call__(self, ids: jax.Array):\n",
    "        return jnp.take(self.A.value, ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d19ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.8423141 , 0.18237865, 0.2271781 , 0.12072563, 0.19181347],\n",
       "       [0.09871054, 0.55314326, 0.12444711, 0.59456205, 0.9594908 ],\n",
       "       [0.6932272 , 0.72409594, 0.31816435, 0.82007146, 0.64102626],\n",
       "       [0.36705065, 0.75454223, 0.36721492, 0.68864214, 0.5837884 ]],      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding(10, 5, rngs=nnx.Rngs(params=0))(jnp.array([0, 2, 3, 9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9feb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nnx.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization. arXiv:1607.06450\n",
    "\n",
    "        y = scale * (x - E[x]) / sqrt(Var[x] + epsilon) + bias\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        *,\n",
    "        use_scale: bool = True,\n",
    "        use_bias: bool = True,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        if use_scale:\n",
    "            self.scale = nnx.Param(rngs.params.uniform((input_dim,)))\n",
    "        else:\n",
    "            self.scale = None\n",
    "        if use_bias:\n",
    "            self.bias = nnx.Param(rngs.params.uniform((input_dim,)))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.epsilon = 1e-9\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True)\n",
    "        result = (x - mean) * lax.rsqrt(var + self.epsilon)\n",
    "        if self.scale is not None:\n",
    "            result *= self.scale.value\n",
    "        if self.bias is not None:\n",
    "            result += self.bias.value\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87781c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9, 7, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LayerNorm(5, use_scale=True, use_bias=True, rngs=nnx.Rngs(params=0))(\n",
    "    jnp.ones((10, 9, 7, 5))\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_dot_product_attention(\n",
    "    q: jax.Array, k: jax.Array, v: jax.Array, *, use_scale: bool = True\n",
    ") -> jax.Array:\n",
    "    \"\"\"\n",
    "    Compute the (scaled) dot-product attention.\n",
    "\n",
    "        Attention(Q, K, V) = softmax(Q K^t / sqrt(d_k)) V\n",
    "\n",
    "    Args:\n",
    "        q: shape (...batch, query_count, qk_dim)\n",
    "        k: shape (...batch, kv_count, qk_dim)\n",
    "        v: shape (...batch, kv_count, v_dim)\n",
    "\n",
    "    Return:\n",
    "        shape (...batch, query_count, v_dim)\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == k.shape[-1]\n",
    "    qk = jnp.einsum(\"...ij,...kj->...ik\", q, k)\n",
    "    if use_scale:\n",
    "        dk = k.shape[-1]\n",
    "        # TODO:\n",
    "        qk *= lax.rsqrt(float(dk))\n",
    "    s = jnn.softmax(qk, axis=-1)\n",
    "    return jnp.einsum(\"...ij,...jk->...ik\", s, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bda3a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 11, 9, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_dot_product_attention(\n",
    "    jnp.zeros((10, 11, 9, 5)),\n",
    "    jnp.zeros((10, 11, 7, 5)),\n",
    "    jnp.zeros((10, 11, 7, 3)),\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9855dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nnx.Module):\n",
    "    \"\"\"\n",
    "    (Scaled) dot-product attention. arXiv:1706.03762\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_q_dim: int,\n",
    "        input_k_dim: int,\n",
    "        input_v_dim: int,\n",
    "        output_dim: int,\n",
    "        qk_dim: int,\n",
    "        *,\n",
    "        use_scale: bool = True,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.use_scale = use_scale\n",
    "        self.W_q = nnx.Param(rngs.params.uniform((input_q_dim, qk_dim)))\n",
    "        self.W_k = nnx.Param(rngs.params.uniform((input_k_dim, qk_dim)))\n",
    "        self.W_v = nnx.Param(rngs.params.uniform((input_v_dim, output_dim)))\n",
    "\n",
    "    def __call__(self, q: jax.Array, k: jax.Array, v: jax.Array):\n",
    "        q = q @ self.W_q\n",
    "        k = k @ self.W_k\n",
    "        v = v @ self.W_v\n",
    "        return generic_dot_product_attention(q, k, v, use_scale=self.use_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06dd1aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 13, 7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DotProductAttention(3, 4, 5, 7, 10, rngs=nnx.Rngs(params=0))(\n",
    "    jnp.zeros((11, 13, 3)), jnp.zeros((11, 13, 4)), jnp.zeros((11, 13, 5))\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nnx.Module):\n",
    "    \"\"\"\n",
    "    A fully connected feed-forward network of depth 2, for using in transformers.\n",
    "\n",
    "        y = ReLU(x W_1 + b_1) W_2 + b_2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dim: int, output_dim: int, hidden_dim: int, *, rngs: nnx.Rngs\n",
    "    ):\n",
    "        self.W_1 = nnx.Param(rngs.params.uniform((input_dim, hidden_dim)))\n",
    "        self.b_1 = nnx.Param(rngs.params.uniform((hidden_dim,)))\n",
    "        self.W_2 = nnx.Param(rngs.params.uniform((hidden_dim, output_dim)))\n",
    "        self.b_2 = nnx.Param(rngs.params.uniform((output_dim,)))\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        y = x @ self.W_1 + self.b_1\n",
    "        y = jnn.relu(y)\n",
    "        return y @ self.W_2 + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36881fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 13, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeedForward(3, 4, 5, rngs=nnx.Rngs(params=0))(jnp.zeros((10, 13, 3))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroLM(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        qk_dim: int,\n",
    "        hidden_dim: int,\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.token_embed = Embedding(vocab_size, embed_dim, rngs=rngs)\n",
    "        self.embed_normalization = LayerNorm(embed_dim, rngs=rngs)\n",
    "        self.attention = DotProductAttention(\n",
    "            embed_dim, embed_dim, embed_dim, embed_dim, qk_dim, rngs=rngs\n",
    "        )\n",
    "        self.feed_forward = FeedForward(embed_dim, embed_dim, hidden_dim, rngs=rngs)\n",
    "        self.lm_head = Linear(embed_dim, vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        # seq_len = x.shape[-1]\n",
    "\n",
    "        # Token id --> embedding\n",
    "        x = self.token_embed(x)\n",
    "        # normalize each embedded token\n",
    "        x_norm = self.embed_normalization(x)\n",
    "        x += self.attention(x_norm, x_norm, x_norm)\n",
    "        x_norm = self.embed_normalization(x)\n",
    "        x += self.feed_forward(x_norm)\n",
    "\n",
    "        x_norm = self.embed_normalization(x)\n",
    "        return self.lm_head(x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e05132",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MicroLM(\n",
    "    vocab_size=256, embed_dim=10, qk_dim=10, hidden_dim=10, rngs=nnx.Rngs(params=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ecbbac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
